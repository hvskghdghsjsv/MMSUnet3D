model: Generic_TransUNet_max_ppbp # model name
model_params: # variants
    is_max_bottleneck_transformer: False # TransUNet backbone
    vit_depth: 1 # number of Transformer layer in TransUNet
    max_msda: ''
    is_masked_attn: True  # turn on Transformer decoder
    max_dec_layers: 3 # number of Transformer decoder layers
    is_max_ms: True # using UNet multi-scale feature to update query in Transformer decoder
    max_ms_idxs: [-4, -3, -2] # which scale feature
    max_hidden_dim: 192
    mw: 0.0 # loss only applied onto Transformer decoder, istead of UNet decoder.
    is_max_ds: True # deep-supervision in Transformer decoder
    is_masking: True # use masked-attention
    is_max_hungarian: True # turn on hungarian matching
    num_queries: 20
    is_max_cls: True # turn on mask classification, along with hungarian matching
    is_mhsa_float32: True # turn on float32 (rather than fp16) incase NAN in softmax

crop_size: [64,192,192] # input patch size
max_loss_cal: 'v1'
batch_size: 4
disable_ds: True
initial_lr: 3e-4
optim_name: adamw
lrschedule: warmup_cosine
resume: 'auto'
warmup_epochs: 10
max_num_epochs: 125 # used 8 cards as default
task:  Task008_HepaticVessel
network: 3d_fullres
network_trainer: nnUNetTrainerV2_DDP
hdfs_base: GeTU008_3DTransUNet_decoder_only
